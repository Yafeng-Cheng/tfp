{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yafeng/Documents/tfp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We'll use the following directory to store files we download as well as our\n",
    "# # preprocessed dataset.\n",
    "# CACHE_DIR = os.path.join(os.sep, '/projects/bbr/code_yc/tfp/test_data/tmp', 'radon')\n",
    "\n",
    "\n",
    "# def cache_or_download_file(cache_dir, url_base, filename):\n",
    "#     \"\"\"Read a cached file or download it.\"\"\"\n",
    "#     filepath = os.path.join(cache_dir, filename)\n",
    "#     if tf.gfile.Exists(filepath):\n",
    "#         return filepath\n",
    "#     if not tf.gfile.Exists(cache_dir):\n",
    "#         tf.gfile.MakeDirs(cache_dir)\n",
    "#     url = os.path.join(url_base, filename)\n",
    "#     print(\"Downloading {url} to {filepath}.\".format(url=url, filepath=filepath))\n",
    "#     urllib.request.urlretrieve(url, filepath)\n",
    "#     return filepath\n",
    "\n",
    "\n",
    "# def download_radon_dataset(cache_dir=CACHE_DIR):\n",
    "#     \"\"\"Download the radon dataset and read as Pandas dataframe.\"\"\"\n",
    "#     url_base = 'http://www.stat.columbia.edu/~gelman/arm/examples/radon/'\n",
    "#     # Alternative source:\n",
    "#     # url_base = ('https://raw.githubusercontent.com/pymc-devs/uq_chapter/'\n",
    "#     #             'master/reference/data/')\n",
    "#     srrs2 = pd.read_csv('test_data/srrs2.csv')\n",
    "#     srrs2.rename(columns=str.strip, inplace=True)\n",
    "#     cty = pd.read_csv('test_data/cty.csv')\n",
    "#     cty.rename(columns=str.strip, inplace=True)\n",
    "#     return srrs2, cty\n",
    "\n",
    "\n",
    "# def preprocess_radon_dataset(srrs2, cty, state='MN'):\n",
    "#     \"\"\"Preprocess radon dataset as done in \"Bayesian Data Analysis\" book.\"\"\"\n",
    "#     srrs2 = srrs2[srrs2.state==state].copy()\n",
    "#     cty = cty[cty.st==state].copy()\n",
    "\n",
    "#     # We will now join datasets on Federal Information Processing Standards\n",
    "#     # (FIPS) id, ie, codes that link geographic units, counties and county\n",
    "#     # equivalents. http://jeffgill.org/Teaching/rpqm_9.pdf\n",
    "#     srrs2['fips'] = 1000 * srrs2.stfips + srrs2.cntyfips\n",
    "#     cty['fips'] = 1000 * cty.stfips + cty.ctfips\n",
    "\n",
    "#     df = srrs2.merge(cty[['fips', 'Uppm']], on='fips')\n",
    "#     df = df.drop_duplicates(subset='idnum')\n",
    "#     df = df.rename(index=str, columns={'Uppm': 'uranium_ppm'})\n",
    "\n",
    "#     # For any missing or invalid activity readings, we'll use a value of `0.1`.\n",
    "#     df['radon'] = df.activity.apply(lambda x: x if x > 0. else 0.1)\n",
    "\n",
    "#     # Remap categories to start from 0 and end at max(category).\n",
    "#     county_name = sorted(df.county.unique())\n",
    "#     df['county'] = df.county.astype(\n",
    "#         pd.api.types.CategoricalDtype(categories=county_name)).cat.codes\n",
    "#     county_name = map(str.strip, county_name)\n",
    "\n",
    "#     df['radon'] = df['radon'].apply(np.log)\n",
    "#     df['log_uranium_ppm'] = df['uranium_ppm'].apply(np.log) \n",
    "#     df = df[['radon', 'floor', 'county', 'log_uranium_ppm']]\n",
    "\n",
    "#     return df, county_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.gfilea.Open(os.path.join(CACHE_DIR, 'radon.csv'), 'w') as f:\n",
    "#     radon.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter('ignore')\n",
    "#     tf.reset_default_graph()\n",
    "#     try:\n",
    "#         sess.close()\n",
    "#     except:\n",
    "#         pass\n",
    "#     sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_scale_transform = lambda y: np.log(y)  # Not using TF here.\n",
    "fwd_scale_transform = tf.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _make_weights_hyper_prior(dtype):\n",
    "#     \"\"\"Returns a `len(log_uranium_ppm)` batch of univariate Normal.\"\"\"\n",
    "#     raw_prior_scale = tf.get_variable(\n",
    "#         name='raw_hyper_prior_scale',\n",
    "#         initializer=np.array(inv_scale_transform(1.), dtype=dtype))\n",
    "#     return tfp.distributions.InverseGamma(\n",
    "#         rate=fwd_scale_transform(raw_prior_scale),\n",
    "#         concentration=fwd_scale_transform(raw_prior_scale))\n",
    "\n",
    "\n",
    "# make_weights_hyper_prior = tf.make_template(\n",
    "#     name_='make_weights_hyper_prior', func_=_make_weights_hyper_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _make_weights_prior(num_counties, dtype):\n",
    "#     \"\"\"Returns a `len(log_uranium_ppm)` batch of univariate Normal.\"\"\"\n",
    "#     raw_prior_scale = tf.get_variable(\n",
    "#         name='raw_prior_scale',\n",
    "#         initializer=np.array(inv_scale_transform(1.), dtype=dtype))\n",
    "#     return tfp.distributions.Independent(\n",
    "#         tfp.distributions.Normal(\n",
    "#             loc=tf.zeros(num_counties, dtype=dtype),\n",
    "#             scale=fwd_scale_transform(raw_prior_scale)),\n",
    "#         reinterpreted_batch_ndims=1)\n",
    "\n",
    "\n",
    "# make_weights_prior = tf.make_template(\n",
    "#     name_='make_weights_prior', func_=_make_weights_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _make_radon_likelihood(\n",
    "#     random_effect_weights, floor, county,log_county_uranium_ppm, init_radon_stddev\n",
    "# ):\n",
    "#     raw_likelihood_scale = tf.get_variable(\n",
    "#         name='raw_likelihood_scale',\n",
    "#         initializer=np.array(\n",
    "#             inv_scale_transform(init_radon_stddev), dtype=dtype))\n",
    "#     fixed_effect_weights = tf.get_variable(\n",
    "#         name='fixed_effect_weights', initializer=np.array([0., 1.], dtype=dtype))\n",
    "#     fixed_effects = fixed_effect_weights[0] + fixed_effect_weights[1] * floor\n",
    "#     random_effects = tf.gather(\n",
    "#         random_effect_weights * log_county_uranium_ppm,\n",
    "#         indices=tf.to_int32(county),\n",
    "#         axis=-1)\n",
    "#     linear_predictor = fixed_effects + random_effects\n",
    "#     return tfp.distributions.Normal(\n",
    "#         #rate=linear_predictor, concentration=fwd_scale_transform(raw_likelihood_scale))\n",
    "#         loc=linear_predictor, scale=fwd_scale_transform(raw_likelihood_scale))\n",
    "\n",
    "\n",
    "# make_radon_likelihood = tf.make_template(\n",
    "#     name_='make_radon_likelihood', func_=_make_radon_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def joint_log_prob(\n",
    "#     random_effect_weights, radon, floor, county,log_county_uranium_ppm, dtype\n",
    "# ):\n",
    "#     num_counties = len(log_county_uranium_ppm)\n",
    "#     rv_weights = make_weights_prior(num_counties, dtype)\n",
    "#     rv_radon = make_radon_likelihood(\n",
    "#         random_effect_weights,\n",
    "#         floor,\n",
    "#         county,\n",
    "#         log_county_uranium_ppm,\n",
    "#         init_radon_stddev=radon.std())\n",
    "#     return (\n",
    "#         rv_weights.log_prob(random_effect_weights)+ \\\n",
    "#         tf.reduce_sum(rv_radon.log_prob(radon), axis=-1)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify unnormalized posterior.\n",
    "\n",
    "# dtype = np.float32\n",
    "\n",
    "# log_county_uranium_ppm = radon[\n",
    "#     ['county', 'log_uranium_ppm']].drop_duplicates().values[:, 1]\n",
    "# log_county_uranium_ppm = log_county_uranium_ppm.astype(dtype)\n",
    "\n",
    "# def unnormalized_posterior_log_prob(random_effect_scale,random_effect_weights):\n",
    "#     return joint_log_prob2(\n",
    "#         my_sigma = random_effect_scale, \n",
    "#         my_gamma=random_effect_weights,\n",
    "#         radon=dtype(radon.radon.values),\n",
    "#         floor=dtype(radon.floor.values),\n",
    "#         county=np.int32(radon.county.values),\n",
    "#         log_county_uranium_ppm=log_county_uranium_ppm,\n",
    "#         dtype=dtype\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set-up E-step.\n",
    "\n",
    "# step_size_hmc = tf.get_variable(\n",
    "#     'step_size_hmc',\n",
    "#     initializer=np.array(0.2, dtype=dtype),\n",
    "#     trainable=False)\n",
    "\n",
    "# hmc = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "#     target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "#     num_leapfrog_steps=2,\n",
    "#     step_size=step_size_hmc,\n",
    "#     step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(\n",
    "#         num_adaptation_steps=None),\n",
    "#     state_gradients_are_stopped=True)\n",
    "\n",
    "# init_random_weights = tf.placeholder(dtype, shape=[len(log_county_uranium_ppm)])\n",
    "\n",
    "# posterior_random_weights, kernel_results = tfp.mcmc.sample_chain(\n",
    "#     num_results=3,\n",
    "#     num_burnin_steps=0,\n",
    "#     num_steps_between_results=0,\n",
    "#     current_state=init_random_weights,\n",
    "#     kernel=hmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set-up M-step.\n",
    "\n",
    "# loss = -tf.reduce_mean(kernel_results.accepted_results.target_log_prob)\n",
    "\n",
    "# global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "# learning_rate = tf.train.exponential_decay(\n",
    "#     learning_rate=0.1,\n",
    "#     global_step=global_step,\n",
    "#     decay_steps=2,\n",
    "#     decay_rate=0.99)\n",
    "\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab variable handles for diagnostic purposes.\n",
    "\n",
    "# with tf.variable_scope('make_weights_prior', reuse=True):\n",
    "#     prior_scale = fwd_scale_transform(\n",
    "#         tf.get_variable(\n",
    "#         name='raw_prior_scale', dtype=dtype\n",
    "#         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope('make_radon_likelihood', reuse=True):\n",
    "#     likelihood_scale = fwd_scale_transform(tf.get_variable(\n",
    "#         name='raw_likelihood_scale', dtype=dtype))\n",
    "#     fixed_effect_weights = tf.get_variable(\n",
    "#         name='fixed_effect_weights', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_op.run()\n",
    "# w_ = np.zeros([len(log_county_uranium_ppm)], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxiter = int(1500)\n",
    "# num_accepted = 0\n",
    "# num_drawn = 0\n",
    "# for i in range(maxiter):\n",
    "#     [\n",
    "#         _,\n",
    "#         global_step_,\n",
    "#         loss_,\n",
    "#         posterior_random_weights_,\n",
    "#         kernel_results_,\n",
    "#         step_size_hmc_,\n",
    "#         prior_scale_,\n",
    "#         likelihood_scale_,\n",
    "#         fixed_effect_weights_,\n",
    "#     ] = sess.run([\n",
    "#         train_op,\n",
    "#         global_step,\n",
    "#         loss,\n",
    "#         posterior_random_weights,\n",
    "#         kernel_results,\n",
    "#         step_size_hmc,\n",
    "#         prior_scale,\n",
    "#         likelihood_scale,\n",
    "#         fixed_effect_weights,\n",
    "#     ], feed_dict={init_random_weights: w_})\n",
    "#     w_ = posterior_random_weights_[-1, :]\n",
    "#     num_accepted += kernel_results_.is_accepted.sum()\n",
    "#     num_drawn += kernel_results_.is_accepted.size\n",
    "#     acceptance_rate = num_accepted / num_drawn\n",
    "#     if i % 100 == 0 or i == maxiter - 1:\n",
    "#         print(\n",
    "#             'global_step:{:>4}  loss:{: 9.3f}  acceptance:{:.4f}  '\n",
    "#             'step_size:{:.4f}  prior_scale:{:.4f}  likelihood_scale:{:.4f}  '\n",
    "#             'fixed_effect_weights:{}'.format(\n",
    "#                 global_step_, loss_.mean(), acceptance_rate, step_size_hmc_,\n",
    "#                 prior_scale_, likelihood_scale_, fixed_effect_weights_))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior_random_weights_final, kernel_results_final = tfp.mcmc.sample_chain(\n",
    "#     num_results=int(15e3),\n",
    "#     num_burnin_steps=int(1e3),\n",
    "#     current_state=init_random_weights,\n",
    "#     kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "#       target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "#       num_leapfrog_steps=2,\n",
    "#       step_size=step_size_hmc))\n",
    "\n",
    "# [\n",
    "#     posterior_random_weights_final_,\n",
    "#     kernel_results_final_,\n",
    "# ] = sess.run([\n",
    "#     posterior_random_weights_final,\n",
    "#     kernel_results_final,\n",
    "# ], feed_dict={init_random_weights: w_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radon.radon.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tensors):\n",
    "    \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
    "    Args:\n",
    "    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
    "      `namedtuple` or combinations thereof.\n",
    " \n",
    "    Returns:\n",
    "      ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
    "        `EagerTensor`s replaced by Numpy `ndarray`s.\n",
    "    \"\"\"\n",
    "    if tf.executing_eagerly():\n",
    "        return tf.contrib.framework.nest.pack_sequence_as(\n",
    "            tensors,\n",
    "            [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
    "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
    "    return sess.run(tensors)\n",
    "\n",
    "\n",
    "class _TFColor(object):\n",
    "    \"\"\"Enum of colors used in TF docs.\"\"\"\n",
    "    red = '#F15854'\n",
    "    blue = '#5DA5DA'\n",
    "    orange = '#FAA43A'\n",
    "    green = '#60BD68'\n",
    "    pink = '#F17CB0'\n",
    "    brown = '#B2912F'\n",
    "    purple = '#B276B2'\n",
    "    yellow = '#DECF3F'\n",
    "    gray = '#4D4D4D'\n",
    "    def __getitem__(self, i):\n",
    "        return [\n",
    "            self.red,\n",
    "            self.orange,\n",
    "            self.green,\n",
    "            self.blue,\n",
    "            self.pink,\n",
    "            self.brown,\n",
    "            self.purple,\n",
    "            self.yellow,\n",
    "            self.gray,\n",
    "        ][i % 9]\n",
    "TFColor = _TFColor()\n",
    "\n",
    "def session_options(enable_gpu_ram_resizing=True, enable_xla=False):\n",
    "    \"\"\"\n",
    "    Allowing the notebook to make use of GPUs if they're available.\n",
    "    \n",
    "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear \n",
    "    algebra that optimizes TensorFlow computations.\n",
    "    \"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.log_device_placement = True\n",
    "    if enable_gpu_ram_resizing:\n",
    "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
    "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
    "        config.gpu_options.allow_growth = True\n",
    "    if enable_xla:\n",
    "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
    "        config.graph_options.optimizer_options.global_jit_level = (\n",
    "            tf.OptimizerOptions.ON_1)\n",
    "    return config\n",
    "\n",
    "\n",
    "def reset_sess(config=None):\n",
    "    \"\"\"\n",
    "    Convenience function to create the TF graph & session or reset them.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = session_options(enable_gpu_ram_resizing=True, enable_xla=False)\n",
    "    global sess\n",
    "    tf.reset_default_graph()\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "reset_sess()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radon</th>\n",
       "      <th>floor</th>\n",
       "      <th>county</th>\n",
       "      <th>log_uranium_ppm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35849128461</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.689048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35849128461</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.689048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39313342971440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.689048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220265</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.689048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>290488496652474</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.847313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             radon  floor  county  log_uranium_ppm\n",
       "0      35849128461      1       0        -0.689048\n",
       "1      35849128461      0       0        -0.689048\n",
       "2   39313342971440      0       0        -0.689048\n",
       "3           220265      0       0        -0.689048\n",
       "4  290488496652474      0       1        -0.847313"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# radon, county_name = preprocess_radon_dataset(*download_radon_dataset())\n",
    "radon = pd.read_csv('test_data/tmp/radon/radon.csv')\n",
    "radon['radon'] = (np.round(np.exp(radon.radon),1)*10).astype(int)\n",
    "radon.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tfd = tfp.distributions\n",
    "dtype = np.float32\n",
    "\n",
    "log_county_uranium_ppm = radon[\n",
    "    ['county', 'log_uranium_ppm']].drop_duplicates().values[:, 1]\n",
    "# log_county_uranium_ppm = log_county_uranium_ppm.astype(dtype)\n",
    "\n",
    "num_counties = len(log_county_uranium_ppm)\n",
    "# county = np.int32(radon.county.values)\n",
    "# floor = dtype(radon.floor.values)\n",
    "# radon = dtype(radon.radon.values)\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    tf.ones([num_counties], dtype=tf.float32, name=\"init_sigma\"),\n",
    "    tf.ones([num_counties], dtype=tf.float32, name=\"init_gamma\"),\n",
    "    tf.ones([2], dtype=tf.float32, name=\"init_beta\")\n",
    "]\n",
    "\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Exp(),       # Maps a positive real to R.\n",
    "    tfp.bijectors.Identity(),       # Maps a positive real to R.\n",
    "    tfp.bijectors.Identity(),   # Maps [0,1] to R.  \n",
    "]\n",
    "\n",
    "def joint_log_prob2(\n",
    "    my_sigma, my_gamma, my_beta, \n",
    "    floor = dtype(radon.floor.values), radon = dtype(radon.radon.values), \n",
    "    county = np.int32(radon.county.values), log_county_uranium_ppm = log_county_uranium_ppm.astype(dtype)\n",
    "):\n",
    "    num_counties = len(log_county_uranium_ppm)\n",
    "    \n",
    "#     fixed_effect_weights = tf.get_variable(\n",
    "#         name='fixed_effect_weights', initializer=np.array([0., 1.], dtype=dtype))\n",
    "    \n",
    "#     raw_hyper_prior_scale = tf.get_variable(\n",
    "#         name='raw_hyper_prior_scale',\n",
    "#         initializer=np.ones(shape=num_counties, dtype=dtype))\n",
    "    \n",
    "    rv_sigma = tfd.Independent(\n",
    "        tfd.InverseGamma(\n",
    "            concentration = tf.ones(num_counties, dtype=tf.float32),\n",
    "            rate = tf.ones(num_counties, dtype=tf.float32)\n",
    "    ),reinterpreted_batch_ndims=1)\n",
    "    \n",
    "    rv_gamma = tfp.distributions.Independent(\n",
    "        tfp.distributions.Normal(\n",
    "            loc=tf.zeros(num_counties, dtype=tf.float32),\n",
    "            scale=my_sigma),\n",
    "        reinterpreted_batch_ndims=1)\n",
    "    \n",
    "    rv_beta = tfp.distributions.Independent(\n",
    "        tfp.distributions.Normal(\n",
    "            loc=tf.zeros([2], dtype=tf.float32),\n",
    "            scale=tf.constant([10,10], dtype=tf.float32)),\n",
    "        reinterpreted_batch_ndims=1)\n",
    "    \n",
    "    fixed_effects = my_beta[0] + my_beta[1] * floor\n",
    "    \n",
    "    random_effects = tf.gather(\n",
    "        my_gamma * log_county_uranium_ppm,\n",
    "        indices=tf.to_int32(county),\n",
    "        axis=-1)\n",
    "    linear_predictor = fixed_effects + random_effects\n",
    "    \n",
    "    \n",
    "    lambda_ = tf.math.exp(linear_predictor)\n",
    "    rv_radon = tfd.Poisson(rate=lambda_)\n",
    "    \n",
    "    return (\n",
    "        rv_beta.log_prob(my_beta)+\\\n",
    "        rv_sigma.log_prob(my_sigma)+\\\n",
    "        rv_gamma.log_prob(my_gamma)+\\\n",
    "        tf.reduce_sum(rv_radon.log_prob(radon), axis=-1)\n",
    "    )\n",
    "\n",
    "def unnormalized_log_posterior(my_sigma, my_gamma, my_beta):\n",
    "    return joint_log_prob2(\n",
    "        my_sigma, my_gamma, my_beta, floor, radon, county, log_county_uranium_ppm\n",
    "    )\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "    step_size = tf.get_variable(\n",
    "        name='step_size',\n",
    "        initializer=tf.constant([0.0001], dtype=tf.float32),#np.array(0.2, dtype=dtype),\n",
    "        trainable=False,\n",
    "        use_resource=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "[\n",
    "    sigma_samples,\n",
    "    gamma_samples,\n",
    "    beta_sample\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=100,\n",
    "    num_burnin_steps=20,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=tfp.mcmc.TransformedTransitionKernel(\n",
    "        inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=joint_log_prob2,\n",
    "            num_leapfrog_steps=2,\n",
    "            step_size=step_size,\n",
    "            step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(),\n",
    "            state_gradients_are_stopped=True),\n",
    "        bijector=unconstraining_bijectors))\n",
    "\n",
    "# Initialize any created variables.\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate: 0.0\n",
      "final step size: 3.0299492209451273e-05\n"
     ]
    }
   ],
   "source": [
    "evaluate(init_g)\n",
    "evaluate(init_l)\n",
    "[\n",
    "    sigma_samples_,\n",
    "    gamma_samples_,\n",
    "    beta_sample_,\n",
    "    kernel_results_\n",
    "] = evaluate([\n",
    "    sigma_samples,\n",
    "    gamma_samples,\n",
    "    beta_sample,\n",
    "    kernel_results\n",
    "])\n",
    "\n",
    "    \n",
    "print(\"acceptance rate: {}\".format(\n",
    "    kernel_results_.inner_results.is_accepted.mean()))\n",
    "print(\"final step size: {}\".format(\n",
    "    kernel_results_.inner_results.extra.step_size_assign[-2000:].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(28):\n",
    "#     plt.plot(gamma_samples_[:,i])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(sigma_samples_.shape[1]):\n",
    "#     plt.figure(figsize=(12.5, 15))\n",
    "#     #histogram of the samples:\n",
    "\n",
    "#     ax = plt.subplot(311)\n",
    "# #     ax.set_autoscaley_on(False)\n",
    "\n",
    "#     plt.hist(\n",
    "#         sigma_samples_[:,i], histtype='stepfilled', bins=50, alpha=0.85,\n",
    "#         label=r\"posterior of $\\lambda_1$\", color=TFColor[0], density=True)\n",
    "#     plt.legend(loc=\"upper left\")\n",
    "#     plt.title(r\"\"\"Posterior distributions of the variables $\\lambda_1,\\;\\lambda_2,\\;\\tau$\"\"\")\n",
    "# #     plt.xlim([15, 30])\n",
    "#     plt.xlabel(r\"$\\lambda_1$ value\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_sample = len(sigma_samples_[-2000:,0])\n",
    "# y_hat=[0]*n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radon_ = radon.iloc[0,0]\n",
    "# floor_ = radon.iloc[0,1]\n",
    "# county_ = int(radon.iloc[0,2])\n",
    "# log_uranium_ppm_ = radon.iloc[0,3]\n",
    "\n",
    "# for i in range(n_sample):\n",
    "#     lambda_i = beta_sample_[-2000+i, ]\n",
    "#     lambda_i = lambda_i[0]+lambda_i[1]*floor_\n",
    "#     lambda_i = lambda_i+ log_uranium_ppm_*np.random.normal(0, gamma_samples_[-2000+i,county_])\n",
    "    \n",
    "#     y_hat[i] = np.random.poisson(lam=lambda_i, size = 1)\n",
    "# plt.hist(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma_samples_[:,int(county[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
