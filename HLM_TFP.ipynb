{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yafeng/Documents/tfp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the following directory to store files we download as well as our\n",
    "# preprocessed dataset.\n",
    "CACHE_DIR = os.path.join(os.sep, '/projects/bbr/code_yc/tfp/test_data/tmp', 'radon')\n",
    "\n",
    "\n",
    "def cache_or_download_file(cache_dir, url_base, filename):\n",
    "    \"\"\"Read a cached file or download it.\"\"\"\n",
    "    filepath = os.path.join(cache_dir, filename)\n",
    "    if tf.gfile.Exists(filepath):\n",
    "        return filepath\n",
    "    if not tf.gfile.Exists(cache_dir):\n",
    "        tf.gfile.MakeDirs(cache_dir)\n",
    "    url = os.path.join(url_base, filename)\n",
    "    print(\"Downloading {url} to {filepath}.\".format(url=url, filepath=filepath))\n",
    "    urllib.request.urlretrieve(url, filepath)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def download_radon_dataset(cache_dir=CACHE_DIR):\n",
    "    \"\"\"Download the radon dataset and read as Pandas dataframe.\"\"\"\n",
    "    url_base = 'http://www.stat.columbia.edu/~gelman/arm/examples/radon/'\n",
    "    # Alternative source:\n",
    "    # url_base = ('https://raw.githubusercontent.com/pymc-devs/uq_chapter/'\n",
    "    #             'master/reference/data/')\n",
    "    srrs2 = pd.read_csv('test_data/srrs2.csv')\n",
    "    srrs2.rename(columns=str.strip, inplace=True)\n",
    "    cty = pd.read_csv('test_data/cty.csv')\n",
    "    cty.rename(columns=str.strip, inplace=True)\n",
    "    return srrs2, cty\n",
    "\n",
    "\n",
    "def preprocess_radon_dataset(srrs2, cty, state='MN'):\n",
    "    \"\"\"Preprocess radon dataset as done in \"Bayesian Data Analysis\" book.\"\"\"\n",
    "    srrs2 = srrs2[srrs2.state==state].copy()\n",
    "    cty = cty[cty.st==state].copy()\n",
    "\n",
    "    # We will now join datasets on Federal Information Processing Standards\n",
    "    # (FIPS) id, ie, codes that link geographic units, counties and county\n",
    "    # equivalents. http://jeffgill.org/Teaching/rpqm_9.pdf\n",
    "    srrs2['fips'] = 1000 * srrs2.stfips + srrs2.cntyfips\n",
    "    cty['fips'] = 1000 * cty.stfips + cty.ctfips\n",
    "\n",
    "    df = srrs2.merge(cty[['fips', 'Uppm']], on='fips')\n",
    "    df = df.drop_duplicates(subset='idnum')\n",
    "    df = df.rename(index=str, columns={'Uppm': 'uranium_ppm'})\n",
    "\n",
    "    # For any missing or invalid activity readings, we'll use a value of `0.1`.\n",
    "    df['radon'] = df.activity.apply(lambda x: x if x > 0. else 0.1)\n",
    "\n",
    "    # Remap categories to start from 0 and end at max(category).\n",
    "    county_name = sorted(df.county.unique())\n",
    "    df['county'] = df.county.astype(\n",
    "        pd.api.types.CategoricalDtype(categories=county_name)).cat.codes\n",
    "    county_name = map(str.strip, county_name)\n",
    "\n",
    "    df['radon'] = df['radon'].apply(np.log)\n",
    "    df['log_uranium_ppm'] = df['uranium_ppm'].apply(np.log) \n",
    "    df = df[['radon', 'floor', 'county', 'log_uranium_ppm']]\n",
    "\n",
    "    return df, county_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# radon, county_name = preprocess_radon_dataset(*download_radon_dataset())\n",
    "radon = pd.read_csv('test_data/tmp/radon/radon.csv')\n",
    "\n",
    "radon['radon'] = (np.round(np.exp(radon.radon),1)*10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.698429614875686e+18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radon.radon.values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/projects/bbr/code_yc/tfp/test_data/tmp/radon'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.gfile.Open(os.path.join(CACHE_DIR, 'radon.csv'), 'w') as f:\n",
    "#     radon.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    tf.reset_default_graph()\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_scale_transform = lambda y: np.log(y)  # Not using TF here.\n",
    "fwd_scale_transform = tf.exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_weights_prior(num_counties, dtype):\n",
    "    \"\"\"Returns a `len(log_uranium_ppm)` batch of univariate Normal.\"\"\"\n",
    "    raw_prior_scale = tf.get_variable(\n",
    "        name='raw_prior_scale',\n",
    "        initializer=np.array(inv_scale_transform(1.), dtype=dtype))\n",
    "    return tfp.distributions.Independent(\n",
    "        tfp.distributions.Normal(\n",
    "            loc=tf.zeros(num_counties, dtype=dtype),\n",
    "            scale=fwd_scale_transform(raw_prior_scale)),\n",
    "        reinterpreted_batch_ndims=1)\n",
    "\n",
    "\n",
    "make_weights_prior = tf.make_template(\n",
    "    name_='make_weights_prior', func_=_make_weights_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_radon_likelihood(\n",
    "    random_effect_weights, floor, county,log_county_uranium_ppm, init_radon_stddev\n",
    "):\n",
    "    raw_likelihood_scale = tf.get_variable(\n",
    "        name='raw_likelihood_scale',\n",
    "        initializer=np.array(\n",
    "            inv_scale_transform(init_radon_stddev), dtype=dtype))\n",
    "    fixed_effect_weights = tf.get_variable(\n",
    "        name='fixed_effect_weights', initializer=np.array([0., 1.], dtype=dtype))\n",
    "    fixed_effects = fixed_effect_weights[0] + fixed_effect_weights[1] * floor\n",
    "    random_effects = tf.gather(\n",
    "        random_effect_weights * log_county_uranium_ppm,\n",
    "        indices=tf.to_int32(county),\n",
    "        axis=-1)\n",
    "    linear_predictor = fixed_effects + random_effects\n",
    "    return tfp.distributions.Normal(\n",
    "        loc=linear_predictor, scale=fwd_scale_transform(raw_likelihood_scale))\n",
    "\n",
    "\n",
    "make_radon_likelihood = tf.make_template(\n",
    "    name_='make_radon_likelihood', func_=_make_radon_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_prob(\n",
    "    random_effect_weights, radon, floor, county,log_county_uranium_ppm, dtype\n",
    "):\n",
    "    num_counties = len(log_county_uranium_ppm)\n",
    "    rv_weights = make_weights_prior(num_counties, dtype)\n",
    "    rv_radon = make_radon_likelihood(\n",
    "        random_effect_weights,\n",
    "        floor,\n",
    "        county,\n",
    "        log_county_uranium_ppm,\n",
    "        init_radon_stddev=radon.std())\n",
    "    return (\n",
    "        rv_weights.log_prob(random_effect_weights)+ \\\n",
    "        tf.reduce_sum(rv_radon.log_prob(radon), axis=-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify unnormalized posterior.\n",
    "\n",
    "dtype = np.float32\n",
    "\n",
    "log_county_uranium_ppm = radon[\n",
    "    ['county', 'log_uranium_ppm']].drop_duplicates().values[:, 1]\n",
    "log_county_uranium_ppm = log_county_uranium_ppm.astype(dtype)\n",
    "\n",
    "def unnormalized_posterior_log_prob(random_effect_weights):\n",
    "    return joint_log_prob(\n",
    "        random_effect_weights=random_effect_weights,\n",
    "        radon=dtype(radon.radon.values),\n",
    "        floor=dtype(radon.floor.values),\n",
    "        county=np.int32(radon.county.values),\n",
    "        log_county_uranium_ppm=log_county_uranium_ppm,\n",
    "        dtype=dtype\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tfp/lib/python3.6/site-packages/numpy/core/_methods.py:122: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims)\n",
      "/anaconda3/envs/tfp/lib/python3.6/site-packages/numpy/core/_methods.py:122: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims)\n"
     ]
    }
   ],
   "source": [
    "# Set-up E-step.\n",
    "\n",
    "step_size_hmc = tf.get_variable(\n",
    "    'step_size_hmc',\n",
    "    initializer=np.array(0.2, dtype=dtype),\n",
    "    trainable=False)\n",
    "\n",
    "hmc = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "    target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "    num_leapfrog_steps=2,\n",
    "    step_size=step_size_hmc,\n",
    "    step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(\n",
    "        num_adaptation_steps=None),\n",
    "    state_gradients_are_stopped=True)\n",
    "\n",
    "init_random_weights = tf.placeholder(dtype, shape=[len(log_county_uranium_ppm)])\n",
    "\n",
    "posterior_random_weights, kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=3,\n",
    "    num_burnin_steps=0,\n",
    "    num_steps_between_results=0,\n",
    "    current_state=init_random_weights,\n",
    "    kernel=hmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up M-step.\n",
    "\n",
    "loss = -tf.reduce_mean(kernel_results.accepted_results.target_log_prob)\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    learning_rate=0.1,\n",
    "    global_step=global_step,\n",
    "    decay_steps=2,\n",
    "    decay_rate=0.99)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab variable handles for diagnostic purposes.\n",
    "\n",
    "with tf.variable_scope('make_weights_prior', reuse=True):\n",
    "    prior_scale = fwd_scale_transform(\n",
    "        tf.get_variable(\n",
    "        name='raw_prior_scale', dtype=dtype\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('make_radon_likelihood', reuse=True):\n",
    "    likelihood_scale = fwd_scale_transform(tf.get_variable(\n",
    "        name='raw_likelihood_scale', dtype=dtype))\n",
    "    fixed_effect_weights = tf.get_variable(\n",
    "        name='fixed_effect_weights', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op.run()\n",
    "w_ = np.zeros([len(log_county_uranium_ppm)], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step:   1  loss:      inf  acceptance:0.0000  step_size:0.1941  prior_scale:1.0000  likelihood_scale:inf  fixed_effect_weights:[0. 1.]\n",
      "global_step:  15  loss:      nan  acceptance:0.0000  step_size:0.1278  prior_scale:nan  likelihood_scale:nan  fixed_effect_weights:[nan nan]\n"
     ]
    }
   ],
   "source": [
    "maxiter = int(15)\n",
    "num_accepted = 0\n",
    "num_drawn = 0\n",
    "for i in range(maxiter):\n",
    "    [\n",
    "        _,\n",
    "        global_step_,\n",
    "        loss_,\n",
    "        posterior_random_weights_,\n",
    "        kernel_results_,\n",
    "        step_size_hmc_,\n",
    "        prior_scale_,\n",
    "        likelihood_scale_,\n",
    "        fixed_effect_weights_,\n",
    "    ] = sess.run([\n",
    "        train_op,\n",
    "        global_step,\n",
    "        loss,\n",
    "        posterior_random_weights,\n",
    "        kernel_results,\n",
    "        step_size_hmc,\n",
    "        prior_scale,\n",
    "        likelihood_scale,\n",
    "        fixed_effect_weights,\n",
    "    ], feed_dict={init_random_weights: w_})\n",
    "    w_ = posterior_random_weights_[-1, :]\n",
    "    num_accepted += kernel_results_.is_accepted.sum()\n",
    "    num_drawn += kernel_results_.is_accepted.size\n",
    "    acceptance_rate = num_accepted / num_drawn\n",
    "    if i % 100 == 0 or i == maxiter - 1:\n",
    "        print(\n",
    "            'global_step:{:>4}  loss:{: 9.3f}  acceptance:{:.4f}  '\n",
    "            'step_size:{:.4f}  prior_scale:{:.4f}  likelihood_scale:{:.4f}  '\n",
    "            'fixed_effect_weights:{}'.format(\n",
    "                global_step_, loss_.mean(), acceptance_rate, step_size_hmc_,\n",
    "                prior_scale_, likelihood_scale_, fixed_effect_weights_))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tfp/lib/python3.6/site-packages/numpy/core/_methods.py:122: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims)\n",
      "/anaconda3/envs/tfp/lib/python3.6/site-packages/numpy/core/_methods.py:122: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-16a588deb9e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mposterior_random_weights_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mkernel_results_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m ], feed_dict={init_random_weights: w_})\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/tfp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tfp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tfp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tfp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tfp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tfp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "posterior_random_weights_final, kernel_results_final = tfp.mcmc.sample_chain(\n",
    "    num_results=int(15e3),\n",
    "    num_burnin_steps=int(1e3),\n",
    "    current_state=init_random_weights,\n",
    "    kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "      target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "      num_leapfrog_steps=2,\n",
    "      step_size=step_size_hmc))\n",
    "\n",
    "[\n",
    "    posterior_random_weights_final_,\n",
    "    kernel_results_final_,\n",
    "] = sess.run([\n",
    "    posterior_random_weights_final,\n",
    "    kernel_results_final,\n",
    "], feed_dict={init_random_weights: w_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon.radon.hist(bins=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
