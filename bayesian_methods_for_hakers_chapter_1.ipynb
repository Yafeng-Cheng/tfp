{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tensors):\n",
    "    \"\"\"Evaluates Tensor or EagerTensor to Numpy `ndarray`s.\n",
    "    Args:\n",
    "    tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`,\n",
    "      `namedtuple` or combinations thereof.\n",
    " \n",
    "    Returns:\n",
    "      ndarrays: Object with same structure as `tensors` except with `Tensor` or\n",
    "        `EagerTensor`s replaced by Numpy `ndarray`s.\n",
    "    \"\"\"\n",
    "    if tf.executing_eagerly():\n",
    "        return tf.contrib.framework.nest.pack_sequence_as(\n",
    "            tensors,\n",
    "            [t.numpy() if tf.contrib.framework.is_tensor(t) else t\n",
    "             for t in tf.contrib.framework.nest.flatten(tensors)])\n",
    "    return sess.run(tensors)\n",
    "\n",
    "class _TFColor(object):\n",
    "    \"\"\"Enum of colors used in TF docs.\"\"\"\n",
    "    red = '#F15854'\n",
    "    blue = '#5DA5DA'\n",
    "    orange = '#FAA43A'\n",
    "    green = '#60BD68'\n",
    "    pink = '#F17CB0'\n",
    "    brown = '#B2912F'\n",
    "    purple = '#B276B2'\n",
    "    yellow = '#DECF3F'\n",
    "    gray = '#4D4D4D'\n",
    "    def __getitem__(self, i):\n",
    "        return [\n",
    "            self.red,\n",
    "            self.orange,\n",
    "            self.green,\n",
    "            self.blue,\n",
    "            self.pink,\n",
    "            self.brown,\n",
    "            self.purple,\n",
    "            self.yellow,\n",
    "            self.gray,\n",
    "        ][i % 9]\n",
    "TFColor = _TFColor()\n",
    "\n",
    "def session_options(enable_gpu_ram_resizing=True, enable_xla=False):\n",
    "    \"\"\"\n",
    "    Allowing the notebook to make use of GPUs if they're available.\n",
    "    \n",
    "    XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear \n",
    "    algebra that optimizes TensorFlow computations.\n",
    "    \"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.log_device_placement = True\n",
    "    if enable_gpu_ram_resizing:\n",
    "        # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
    "        # GPU. Otherwise the colab malloc's all GPU ram.\n",
    "        config.gpu_options.allow_growth = True\n",
    "    if enable_xla:\n",
    "        # Enable on XLA. https://www.tensorflow.org/performance/xla/.\n",
    "        config.graph_options.optimizer_options.global_jit_level = (\n",
    "            tf.OptimizerOptions.ON_1)\n",
    "    return config\n",
    "\n",
    "\n",
    "def reset_sess(config=None):\n",
    "    \"\"\"\n",
    "    Convenience function to create the TF graph & session or reset them.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = session_options(enable_gpu_ram_resizing=True, enable_xla=False)\n",
    "    global sess\n",
    "    tf.reset_default_graph()\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    sess = tf.InteractiveSession(config=config)\n",
    "reset_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon = pd.read_csv('test_data/tmp/radon/radon.csv')\n",
    "radon['radon'] = (np.round(np.exp(radon.radon),1)*10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_data = tf.constant([\n",
    "    13,  24,   8,  24,   7,  35,  14,  11,  15,  11,  22,  22,  11,  57,  \n",
    "    11,  19,  29,   6,  19,  12,  22,  12,  18,  72,  32,   9,   7,  13,  \n",
    "    19,  23,  27,  20,   6,  17,  13,  10,  14,   6,  16,  15,   7,   2,  \n",
    "    15,  15,  19,  70,  49,   7,  53,  22,  21,  31,  19,  11,  18,  20,  \n",
    "    12,  35,  17,  23,  17,   4,   2,  31,  30,  13,  27,   0,  39,  37,   \n",
    "    5,  14,  13,  22,\n",
    "], dtype=tf.float32)\n",
    "count_data_mean = tf.reduce_mean(count_data)\n",
    "n_count_data = tf.shape(count_data)\n",
    "count_of_text_msgs = tf.range(n_count_data[0])\n",
    "\n",
    "# Convert from TF to numpy.\n",
    "\n",
    "[\n",
    "    count_data_, \n",
    "    count_data_mean_,\n",
    "    n_count_data_, \n",
    "    count_of_text_msgs_,\n",
    "] = evaluate([\n",
    "    count_data, \n",
    "    count_data_mean,\n",
    "    n_count_data,\n",
    "    count_of_text_msgs,\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_chain_state = [\n",
    "    tf.to_float(tf.reduce_mean(count_data)) * tf.ones([], dtype=tf.float32, name=\"init_lambda1\"),\n",
    "    tf.to_float(tf.reduce_mean(count_data)) * tf.ones([], dtype=tf.float32, name=\"init_lambda2\"),\n",
    "    0.5 * tf.ones([], dtype=tf.float32, name=\"init_tau\"),\n",
    "]\n",
    "\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Exp(),       # Maps a positive real to R.\n",
    "    tfp.bijectors.Exp(),       # Maps a positive real to R.\n",
    "    tfp.bijectors.Sigmoid(),   # Maps [0,1] to R.  \n",
    "]\n",
    "\n",
    "\n",
    "def joint_log_prob(count_data, lambda_1, lambda_2, tau):\n",
    "    tfd = tfp.distributions\n",
    " \n",
    "    alpha = (1. / tf.reduce_mean(count_data))\n",
    "    rv_lambda_1 = tfd.Exponential(rate=alpha)\n",
    "    rv_lambda_2 = tfd.Exponential(rate=alpha)\n",
    " \n",
    "    rv_tau = tfd.Uniform()\n",
    " \n",
    "\n",
    "    lambda_ = tf.gather(\n",
    "         [lambda_1, lambda_2],\n",
    "         indices=tf.to_int32(tau * tf.to_float(tf.size(count_data)) <= tf.to_float(tf.range(tf.size(count_data)))))\n",
    "    rv_observation = tfd.Poisson(rate=lambda_)\n",
    " \n",
    "    return (\n",
    "         rv_lambda_1.log_prob(lambda_1)\n",
    "         + rv_lambda_2.log_prob(lambda_2)\n",
    "         + rv_tau.log_prob(tau)\n",
    "         + tf.reduce_sum(rv_observation.log_prob(count_data))\n",
    "    )\n",
    "\n",
    "\n",
    "# Define a closure over our joint_log_prob.\n",
    "def unnormalized_log_posterior(lambda1, lambda2, tau):\n",
    "    return joint_log_prob(count_data, lambda1, lambda2, tau)\n",
    "\n",
    "\n",
    "# Initialize the step_size. (It will be automatically adapted.)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "    step_size = tf.get_variable(\n",
    "        name='step_size',\n",
    "        initializer=tf.constant(0.05, dtype=tf.float32),\n",
    "        trainable=False,\n",
    "        use_resource=True\n",
    "    )\n",
    "\n",
    "# Sample from the chain.\n",
    "[\n",
    "    lambda_1_samples,\n",
    "    lambda_2_samples,\n",
    "    posterior_tau,\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=100,\n",
    "    num_burnin_steps=10,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=tfp.mcmc.TransformedTransitionKernel(\n",
    "        inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=unnormalized_log_posterior,\n",
    "            num_leapfrog_steps=2,\n",
    "            step_size=step_size,\n",
    "            step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(),\n",
    "            state_gradients_are_stopped=True),\n",
    "        bijector=unconstraining_bijectors))\n",
    "\n",
    "tau_samples = tf.floor(posterior_tau * tf.to_float(tf.size(count_data)))\n",
    "\n",
    "# tau_samples, lambda_1_samples, lambda_2_samples contain\n",
    "# N samples from the corresponding posterior distribution\n",
    "N = tf.shape(tau_samples)[0]\n",
    "expected_texts_per_day = tf.zeros(n_count_data)\n",
    "\n",
    "\n",
    "# Initialize any created variables.\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate: 0.38\n",
      "final step size: 0.04196721687912941\n"
     ]
    }
   ],
   "source": [
    "evaluate(init_g)\n",
    "evaluate(init_l)\n",
    "[\n",
    "    lambda_1_samples_,\n",
    "    lambda_2_samples_,\n",
    "    tau_samples_,\n",
    "    kernel_results_,\n",
    "    N_,\n",
    "    expected_texts_per_day_,\n",
    "] = evaluate([\n",
    "    lambda_1_samples,\n",
    "    lambda_2_samples,\n",
    "    tau_samples,\n",
    "    kernel_results,\n",
    "    N,\n",
    "    expected_texts_per_day,\n",
    "])\n",
    "\n",
    "    \n",
    "print(\"acceptance rate: {}\".format(\n",
    "    kernel_results_.inner_results.is_accepted.mean()))\n",
    "print(\"final step size: {}\".format(\n",
    "    kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
